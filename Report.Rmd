---
title: Estimating Pro-War and Anti-War Sentiments by State Using Multilevel Regression with Poststratification
author: "Parker Kaukl"
date: 5/17/2022
output:
  pdf_document: default
  html_document:
    df_print: paged
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, fig.height=4, fig.width=6)
# feel free to change fig.height or fig.width here or in specific chunk headings. 
```



```{r, include=FALSE}
library(tidyverse)
library(knitr)
library(lme4)
library(lmerTest)
library(gridExtra)
library(corrplot)
library(GGally)
library(kableExtra)
library(arm)
library(usmap)
library(dplyr)
library(tidycensus)
library(stargazer)
library(coda)
library(rstanarm)
library(bayesplot)
```


```{r}
 load("Workspace.RData") # This is data that had previously been collected from the previous files.
```
## Abstract 

One of the most complex topics in american politics is military intervention. Public opinion on military intervention is historically divided into pro-war and anti-war positions. Understanding which states may be more or less likely to be pro or anti War is important for predicting which areas may have more protests, and inform policy positions for elected officials. This paper will create a model to predict opinions on war using a Frequentist and a Bayesian approach. Then, these models will be used in order to create a Multilevel Regression with Post-Stratification to estimate the public opinion within each state in the United States of America. In both the Bayesian and the Frequentist approaches, our Multilevel Regression with Post-Stratification estimates that the public opinion within each state to believe that War is effective is between 60% and 70%. These results tend to be similar to the public opinion polls on war, before any casualties have been incurred.

\newpage


### Introduction

The decision to go to war is a complex decision that has severe consequences. A report from Brown University estimated that around 900,000 deaths occurred in the War on Terror. These fatalities  include soldiers, civilians, journalists and aid workers.In addition, the U.S.'s War on Terror has incurred a cost of $8 trillion dollars since 2001. Because of these high societal costs, it is not surprising that conflict tends to be a polarizing topic, and a topic that has attracted theories on when conflict may occur. 

One of the first theories of War is the "Just War" theory, by St. Augustine. This posited that conflict should occur if it is for a worthy cause, if conflict is likely to achieve that cause and sponsored by a legitimate government. "Just War" theory also states that conflict should only be used as a last resort, and that conflict should not be disproportionate to the cause, and that non-combatants should be respected. Although this theory is valuable, the discourse on military conflict has been furthered by philosophers, politicians, and theologians over the past centuries. 

However, perhaps the most relevant and important influence on our perception of war has been the United State's recent history of war. A lot of theories and thoughts on war were developed in the aftermath of the Vietnam War. As noted in Richard Haas's book, Intervention, the Vietnam War was an event that informs both our policymakers and citizens on what War is like. Perhaps more importantly, the Vietnam War and the protests related to U.S. involvement introduced the importance of U.S. public opinion to whether we should engage in military conflicts.

Caspar Weinberger, a Secretary of Defense under Ronald Reagan, introduced the Weinberger Doctrine, which stated that force should only be used if there is public or congressional support for military conflict. This was furthered by Warren Christopher, the Secretary of Defense under Bill Clinton, who said that having "a high likelihood of support" of popular and congressional support was a prerequisite for military intervention. Other Politicians have not mentioned public opinion as a prerequisite, but have mentioned that it is desirable to have support for any military intervention. 

The Effects of Public Opinion during Wartime are numerous. As noted by George Gallup, Public Opinion can often coincide with political changes. As stated in _How Important is Public Opinion in Time of War?_, "In addition to conscription, the people of this country have been ahead of their political leaders on virtually all important war-time issues" (Gallup, 441). In addition, Conflict is often likely to not occur, or forces are likely to be withdrawn in the case of public disapproval. This can be seen in a lack of approval for U.S. Intervention in Somalia, and later the U.S.'s lack of military intervention in Rwanda during the Rwanda Genocide. In addition, the mismanagement of military intervention threatens re-election campaigns for politicians. Because of this threat, "Politicians--Whose foremost goal is to gain or retain office- will, in an electoral context, be motivated to support changes in war policy if opinion trends among their constituents offer an incentive to do so" (Lieberfeld, 2008). Public Opinion acts as a political constraint to the use of force, in addition to providing probable policy platforms during upcoming elections. 

For Policymakers, understanding public opinion is crucial to making the correct decision about when it is appropriate to use military force. In addition, for citizens, public opinion can provide a good predictor of future policy decisions. Therefore, this paper will attempt to create a frequentist and a Bayesian model in order to estimate the public opinion based off a series of surveys known as the Chicago Council Survey of American Public Opinion on U.S.Foreign Policy from 2016 and 2017. The Survey question that we will analyze focuses on the public opinion on the effectiveness of war. Using this data, we will estimate the expected value of political support in each U.S. state during that time span. This will be done using a method known as Multilevel Regression with Post-stratification, using data from the American Community Survey. Then, this paper will attempt to validate these models by checking that there are no errors in this model, and doing model validation.  

### Background and Significance

> In this paper, I might want to discuss some of the theories related to predictors of pro vs. anti war sentiment
> I could also mention ancedotal evidence of war, and the details from Gelpi and Haas
> Some Demographics that we expect to be important are things such as Veterans Status, Age, Gender, Education Level, and race. We also may consider certain variables that represent individuals ideology, such as Political Party. We may also consider circumstantial, such as whether their party is perpitrating the war and perceived success of this conflict, to be important. 
> Introduce the diachotomy of Casualty Tolerance and Conflict Tolerance (Hawks, Reluctant Hawks, Timid Hawks, and Doves)

There have been numerous opinion polls and regressions that have been run in order to estimate pro or anti war opinion during the "post cold war" era. There are a few reasons why authors and academics tend to start their analysis of war trends in the "post cold war" era. In general, the individual's personal policy preference on war tends to be very temporal. The cold war provided a very unique circumstance, such as the policy of deterrence and the fear of a global war, that does not necessarily reflect the current circumstances of conflicts in the "post cold war" era. Because of this, our scholarship tends to focus on the "post cold war" era as a separate era to the cold war era. 

According to Gelpi and Weaver, the determination of a citizen's preference is based on that individual's demographics, and the context of the situation. Specifically, individuals tend to be in support of military intervention if they believe that they have the "Right" to attack, and they believe that military intervention will be successful to achieve their preferred policy goals. Many of these contextual results are likely to reflect incredibly temporal events that are vulnerable to political changes. For instance, individuals tend to look towards individuals such as politicians that they trust, and organizations such as NATO. If an organization like NATO, or a president that they trust endorse a military action, individuals are extremely likely to support that military interaction. This was found by both Olga Khazan(2013) and by Gelpi and Weaver(2009). 

In addition, individuals are likely to make choices depending on the Principal Policy Objective, according to Jentleson (1992). This thesis argues that individuals have different values, and that the primary objective of why is going to war is likely to influence the level of political support for intervention. For instance, according to a poll from Jentleson, individuals tend to be more in support of Foreign Policy Restraint missions, as opposed to Internal Political Change missions. Foreign Policy Restraint typically involves the use of military force in order to affect the behavior of a nation, whereas Internal Political Change Missions involve using the military to replace or restructure an existing regime. 

Finally, individuals also tend to support missions that they believe will be successful. As noted by Gelpi and Weaver (2009), public support for military intervention tends to drastically decreasing during periods of failure. For instance, In the aftermath of a failed raid in Somalia in 1993, public support for military intervention dropped by 8 percentage points. In general, individuals tend to support a war when there is evidence that there is some level of success. 

While these circumstantial predictors are important, there are additional variables that might be important in order to predict pro vs. antiwar sentiment. These variables are demographic, and tend to be less dependent on the context of the conflict in determining public opinion. For instance, our evidence of gender suggests that there is a mixed effect of whether women or men are more supportive of conflict. Gelpi and Feaver wrote *Paying the Human Costs of War*, which collects regressions that indicate that there is conflicting evidence about the effect of gender in being pro or antiwar. Men were more supportive of military intervention in Lebanon in 1983. Men also tended to be more supportive of military intervention in Somalia in 1993. In the early 2000s, a general poll reported by Gelpi and Feaver stated that there was no statistical significance in gender in determining whether an individual supports military intervention. However, evidence from polls taken during the Kosovo crisis suggests that Women were more likely to support military intervention through Air strikes in the Kosovo War. Women were also more likely to fall into the category that Gelpi and Feaver refer to as "Timid Hawks". Gelpi and Feaver use this term to classify individuals who are supportive of War in the abstract, but tend to withdraw their support as the war becomes costly, or as casualties from the conflict increase. 

Another variable that is very important to consider is age. Ancedotally, there is evidence that suggests that age can both increase and decrease support for War. Many historians note the fervor that younger men tended to have, and their excitement for World War 1. Meanwhile, Vietnam War Era protests are remembered for the swams of college students who protested against the U.S's involvement in Southeast Asia. However, data tends to provide a conflicted history of who tends to be more pro war. Older individuals were less likely to support a military intervention in Lebanon and in Somalia. However, age did not appear to play any significant role in whether an individual supported intervention in the Kosovo War. 

The role of education in determining pro and anti war stance tends to be very consistent. These estimates tend to suggest that individuals with higher levels of education may be less likely to support war. People were less likely to support a military intervention in Kosovo with higher levels of education. In addition, people with higher levels of education were less likely to support the use of force if the objective of the mission is a matter of national security. However, education did not have a significant change for whether an individual supported military intervention in Yemen or in Lebanon.

There are many different additional variables that might be important in determining which individuals will support or not support a war. For instance, people tend to support a war if that war was initiated by the party that aligns with their own personal politics. For instance, democrats tend to be more supportive of military intervention if a democratic president initiated that war, and republican voters are more likely to support wars initiated during a republican presidency. In addition, individuals who are veterans tend to be less likely to support military intervention, as found by Khazan (2013). The affect of other demographic variables, such as Race, tend to have inconsistent evidence on the direction and the significance of the effect of racial identity on pro and antiwar stance. 



### Data

> Introduce the data from both the ACS and the Chicago Foreign Council. This introduction should include what major questions I am answering, the specificatoins of the data (variables, observations, etc.)
> Do some mild Data Analysis, such as creating graphs, in order to visualize the data.
> Discuss the limitations of this data. For instance, the Census cannot and does not ask questions related to how we feel about political parties, veteran status. Ideally, we would want to create more stratas, related to race, veteran status, political party orientation, etc. However, limitations in the census data, as well as time constraints, would prevent me from doing this. 
> In addition, the CFC data only has data for 4767 observations over 2 years. Because of this smaller sample size, we are restrained from using more 

There are three different datasets that I used in this paper. The Chicago Council Survey was used in order to collect variables, including both the response variable and our explanatory variables. Next, the American Community Survey was used in order to collect population proportions in each state. Finally, the American National Election Studies data from 2020 was used in order to check if our estimates of state level proportions are accurate to future studies that have analyzed the support for military actions. 

The first dataset that I used was the Chicago Council Survey of American Public Opinion on U.S. Foreign Policy. This is an annual survey that asks questions related to the foreign policies in the United States, and the public opinion on these policies. For the purpose of my data, I am using the 2016 and 2017 Chicago Council Survey of American public Opinion. These two years were used because they are both publically available, and they both have a question that is coded the same. The target population of this survey was those living in the U.S. who were non-institutionalized and who are aged over 18. The survey was administered by the KnowledgePanel, which is a probability based web panel designed to be representative of the U.S. results. The administrators of this survey included various checks, such as removing those who answered the test too quick and those who failed the "quality checks". The "quality checks" consisted of questions such as "Pick option 3", which indicates whether a respondent was actually attentive during the survey. 

In order to clean this data, we took multiple steps. First, any cases where there was a missing value were discarded, in order to prevent any bias that might occur by keeping these variables in the dataset. In addition, many of the variables were manipulated in order to create new variables for the purpose of a regression. Initially, there was a variable called *Q8_14*. This variable recorded the respondents answer to the question *How effective do you think each of the following approaches are to achieving the foreign policy goals of the United States- very effective, somewhat effective, not very effective, or not effective at all: Intervening Militarily*. Using this variable, we manipulated it into a new variable called *ProWarBinary*. If this variable is equal to 1, then the respondent believes that military intervention is either very effective or somewhat effective. Meanwhile, if the respondent believes that military intervention is not very effective or not effective at all, then the variable will return with a value of 0. 



V201350<- This is the data set that we use for everything in the ANES dataset

The dataset used is from Chap et al. (2019) , which explored 2014 congressional candidates’ ambiguity on political issues. The authors "hand-coded a random sample of 2012 congressional candidates’ websites, assigning an ambiguity score." A total of 870 websites from 2014 were then automatically scored using Wordscores, a program designed for political textual analysis. 

Wordscores is a textual analysis software that performs spatial regressions and frequency clouds on training texts to analyze the "ambiguity" of political statements by analyzing their distance from actionable positions as compared to adjectival statements about the issues. In the original paper that documented the creation of Wordscores, researchers stated that "because it treats words unequivocally as data, our technique not only allows us to estimate policy positions from political texts written in any language but, uniquely among the methods currently available, it allows us to calculate confidence intervals around these point estimates."

This kind of machine-learning analysis can be hard to interpret because verbs were shown to have much higher clarity scores in policy position pages, while adjectives and pronouns were seen as ambiguous. However, the verbs that were labeled as more clear were not necessarily actionable, such as "do," "can," and modifiers like "will." Relying on training texts (taken from the same genre of material) to code which words are more or less ambiguous can be an unreliable measure, as other ambiguous political communication could just as easily be seen in the training texts. [Footnote 1]

The dataset is multilevel because it captures information both on a candidate and their district. The first level variables pertain to candidates. The candidates name, their party (a binary variable of 1 for Democrats and 0 for Republicans), incumbency (also binary, with 1 indicating incumbency), ideology and ambiguity are included. Ideology is a more specific measure of the candidate’s left-right orientation. Higher (positive) scores indicate more conservative candidates and lower (negative) scores indicate more liberal candidates; this type of ideological measure was done on both candidates and their district to create a 'mismatch' variable. Mismatch is the distance between the candidate’s ideology and the district’s ideology. The response variable is ambiguity. It is an assigned score, produced by the Wordscores software; higher scores indicate greater ambiguity (i.e., less clarity). This score is a standardized measurement of ambiguity, where a measurement of 0 represents that a candidate's ambiguity is equal to the average ambiguity for all candidates. [Footnote 2]

The second level is the congressional district that the candidate is in. Each district is assigned an identification number. The district's ideological lean was measured to indicate whether the district was more liberal or conservative. There are also two "heterogeneity" variables at the district level, which measure the variability within a district's demographic information and ideological lean of their citizens. For both, higher scores imply more heterogeneity among voters.

To clean our data, we reversed the sign of ambiguity. In the original dataset, higher ambiguity scores indicated less ambiguity. This method of measurement was confusing, and therefore we decided to multiply the ambiguity value by negative one, to get a new variable. Because this variable was a z-score, this will only change the direction, but not the magnitude. In addition, we also removed a datapoint. In District 409, some of the values for district level measurements had values that were mathematically impossible, such as having a negative value of Attitude Heterogeneity. A value of 0 indicates that there is complete Attitude Homogeneity, so therefore, a negative value would be impossible. 

```{r, fig.cap="Table One: A Preview of the Dataset and its Variables"}
pol_read <- read.csv("https://raw.githubusercontent.com/proback/BeyondMLR/master/data/ambiguity.csv")
pol <- pol_read %>% filter(distID!=409) %>% mutate(ambiguity_fix = ambiguity*-1) %>% na.omit()
#pol$democrat <- as.factor(pol$democrat)
pol<-pol %>% dplyr::select(-ambiguity)
T1 <- head(pol)       
kable(T1, caption="First 6 rows of the dataset",col.names = c("Name","District ID", "Ideology Score","Issue Pages","Democrat","District Mismatch","Incumbent","Demographic Heterogeneity","Attitude Heterogeneity","District Lean","Ambiguity"),format="pipe") %>% kable_styling(latex_options=c(font_size = 8))
#summary(pol)
```

We start with a correlation plot (Figure 1) and a scatterplot matrix (Figure 2) to analyze the relationships between numeric and continuous variables. As shown in Figure 1, the strongest linear correlation is between ideology and partisanship, which is expected because both are measures of ideology in their own way and (with few exceptions), democratic candidates will fall on the left and Republican candidates on the right. 


```{r, fig.cap="Correlation Plot of Numeric Variables in the Dataset"}
Pol_Corr_Matrix <- select_if(pol, is.numeric)
AAA <- cor(Pol_Corr_Matrix, use = "pairwise.complete.obs")
BBB <- corrplot(AAA)
```

```{r, fig.cap="Correlation Matrix of Continuous Variables in the Dataset"}
pol$democrat<-as.factor(pol$democrat)
pol_matrix <- pol %>% dplyr::select(ambiguity_fix, incumbent, ideology, distLean, mismatch, totalIssuePages)
ggpairs(pol_matrix)
```

Following this, we specifically explore the relationships between ideological variables `ideology`, `party`, and `incumbency`. Figure 3 shows an overall trend of more ambiguous political communications as a candidate becomes more ideologically conservative. Figure 2 shows a more nuanced picture of the relationship between `ideology` and `ambiguity` where slopes differ according to party.The tendency for both parties is to have more ambiguous language for more candidates with moderate ideologies, but the effect is more extreme for Democrats. When we add whether a candidate was an incumbent or challenger in Figure 4, we see that for Republicans `incumbency` has a balancing effect. We see consistent ambiguity scores for Republican incumbents over a range of ideologies, unlike the other three groups. From Figure 5 we can also see that for both parties, challengers have more ambiguous language overall than their incumbent counterparts. This is reflected in Table 1 comparing `mean_ambiguity` for each group. These figures suggest we should consider adding interaction terms for `party` and `ideology`, and `party` and `incumbency`. Figure 6 shows the distribution of ideologies by party. We see overlap near the center of the distribution. This histogram, along with the party/ideology correlation test result showing correlation = -0.6724306 tell us that we should proceed with models including party and evaluate further using AIC and BIC.

```{r, fig.cap="Scatterplot of Communication Ambiguity by Ideology"}

ggplot(pol, aes(x = ideology, y = ambiguity_fix)) + geom_point() + geom_smooth(method = "lm", se = F) + labs(title = "Communication Ambiguity by Ideology", x = "Ideology", y = "Ambiguity") + scale_x_continuous(limits=c(-4,4), breaks=c(-4, -2, 0, 2, 4)) + scale_color_manual(values = c("red", "blue"), labels = c("Republican", "Democrat"))

```

```{r, fig.cap="Scatterplot of Communication Ambiguity by Ideology, Grouped by Partisanship"}

ggplot(pol, aes(x = ideology, y = ambiguity_fix)) + geom_point(aes(color = factor(democrat)), alpha = 0.2) + geom_smooth(method = "lm", se = F, aes(color = factor(democrat))) + labs(title = "Communication Ambiguity by Party and Ideology", x = "Ideology", y = "Ambiguity", col = "Party") + scale_x_continuous(limits=c(-4,4), breaks=c(-4, -2, 0, 2, 4)) + scale_color_manual(values = c("red", "blue"), labels = c("Republican", "Democrat"))

```

```{r, fig.cap="Scatterplot of Communication Ambiguity by Ideology, Grouped by Partisanship and Incumbency"}

ggplot(pol, aes(x = ideology, y = ambiguity_fix)) + geom_point(aes(color = factor(democrat)), alpha = 0.2) + geom_smooth(method = "lm", se = F, aes(color = factor(democrat), linetype = factor(incumbent))) + labs(title = "Communication Ambiguity by Party, Ideology, and Incumbency", x = "Ideology", y = "Ambiguity", col = "Party") + scale_x_continuous(limits=c(-4,4), breaks=c(-4, -2, 0, 2, 4)) + scale_color_manual(values = c("red", "blue"), labels = c("Republican", "Democrat")) + scale_linetype_discrete(name  ="Incumbency", breaks=c("0", "1"), labels=c("Challenger", "Incumbent"))

```

```{r, fig.cap="Ideological Distribution by Party"}

ggplot(pol, aes(x = ideology)) + geom_histogram(aes(fill = factor(democrat))) + labs(title = "Ideological Distribution by Party", x = "Ideology", y = "Count", fill = "Party") + scale_x_continuous(limits=c(-4,4), breaks=c(-4, -2, 0, 2, 4)) + scale_fill_manual(values = c("red", "blue"), labels = c("Republican", "Democrat"))

```

This exploratory analysis of the relationship between party, ideology, and their effect on ambiguity was particularly important due to the strong but incomplete correlation of partisanship and ideology. We felt that using both ideology and party as a fixed effect may cause multicollinearity when we were originally creating our model. However, our correlation tests, plots, and further analysis have suggested that using both variables would not create a multicollinearity problem. In addition, both variables play an important part in profiling a candidate and explaining ambiguity in their messaging. Table 2 shows that incumbent Republicans and Democratic challengers have very similar mean ambiguity scores. 

```{r}

table_1 <- pol %>% group_by(factor(democrat), factor(incumbent)) %>% summarise(mean_ambiguity = mean(ambiguity_fix, na.rm = TRUE), standard_deviation = sd(ambiguity_fix, na.rm = TRUE), min = min(ambiguity_fix, na.rm = TRUE), max = max(ambiguity_fix, na.rm = TRUE), N = n())

 

kable(table_1, col.names=c("Democrat","Incumbent","Mean Ambiguity", "Standard Deviation of Ambiguity","Minimum Ambiguity", "Maximum Ambiguity","Number"),format="pipe",caption="Summary Statistics of Ambiguity, Grouped by Partisanship and Incumbency")

```


One important dimension of the political data set focuses on the political characteristics of the district. Some of these variables are important to predict political ambiguity. For instance, we might predict that districts that are more polarized might have more ambiguity in their politicians. In districts with higher levels of polarization, or in districts that have a greater variety in political ideologies, we might expect that there is a different level of ambiguity in a politician. Therefore, it may be important to observe different district level variables.

First, we will construct numerous plots to observe the different variables at a district leve, in order to understand their distribution and the relationship that these variables have with ambiguity. These resulting figures are shown in Figure 7. District Lean, Attitude Heterogeneity and Demographic Heterogeneity all have a roughly normal distribution. District Demographic Heterogeneity and District Lean both appear to have skews, with District Lean having a skew towards liberal ideologies, and a skew towards districts with more Demographic Heterogeneity. In these graphs, we are able to see a few weaker, general trends. We can see that there is no clear correlation between ambiguity and District Lean, or District Attitude Heterogeneity and Ambiguity. However, the District Demographic Heterogeneity appears to have a relationship with the ambiguity, so we might consider adding that variable.

\newpage

```{r,fig.width=10,fig.height=5, fig.cap="Analyzing Frequency and Trends of District-Level Variables in Relation to Ambiguity"}

pol_level_two<-pol %>%
  group_by(distID) %>%
  summarize(DistdemHeterogeneity=mean(demHeterogeneity),
         DistattHeterogeneity=mean(attHeterogeneity),
         DistAmbiguity=mean(ambiguity_fix),
         DistLean=mean(distLean),
         count=n())

 

h1<-ggplot(pol_level_two,aes(x=DistdemHeterogeneity))+xlab("District Demographic Heterogeneity")+ylab("Frequency")+geom_density(color="blue",fill="blue",alpha=.4)

 

h2<-ggplot(pol_level_two,aes(x=DistattHeterogeneity))+xlab("District Attitude Heterogeneity")+ylab("Frequency")+geom_density(color="blue",fill="blue",alpha=.4)

 

h3<-ggplot(pol_level_two,aes(x=DistLean))+xlab("District Lean")+ylab("Frequency")+geom_density(color="blue",fill="blue",alpha=.4)

 

a1<-ggplot(pol_level_two,aes(x=DistdemHeterogeneity,y=DistAmbiguity))+xlab("District Demographic Heterogeneity")+ylab("Average Ambiguity")+geom_point()+geom_smooth(method="lm",color="red")

 

a2<-ggplot(pol_level_two,aes(x=DistattHeterogeneity,y=DistAmbiguity))+geom_point()+xlab("District Attitude Heterogeneity")+ylab("Average Ambiguity")+geom_smooth(method="lm",color="red")

 

a3<-ggplot(pol_level_two,aes(x=DistLean,y=DistAmbiguity))+geom_point()+xlab("District Lean")+ylab("Average Ambiguity")+geom_smooth(method="lm",color="red")

 

grid.arrange(h1,h2,h3,a1,a2,a3,ncol=3)

```


### Methods 

To evaluate different models, we have used the AIC and BIC values. These values show us how much variability in a model is accounted for, while penalizing the use of additional variables. We decided that the best model minimizes these AIC and BIC terms compared to other models. In addition, the model that we want to choose would investigate the relationships with ambiguity that we have previously identified. We started with a baseline unconditional means model. From there, we decided to include additional terms, as this improved the AIC and BIC of our model. An ideal model for this report would reduce the AIC and BIC relative to other models, while addressing the effect on ambiguity of candidate’s party, ideology, the number of issue pages and demographic heterogeneity. 


With only two observations per district, any random slope would be created from only those two observations and thus any random slope term would be inappropriate to apply to this dataset. Therefore, we choose to use a random effect term for District, instead of any other random effects or slopes. 

First, we identified that we wanted to use a model that included ideology and the incumbent status of the candidate. Then, we additionally recognized that the variable for demographic heterogeneity was important in predicting the ambiguity of a candidate’s position. Figure 7 shows a positive correlation between District Demographic Heterogeneity and Ambiguity. We also found a relationship between the number of issue pages and the ambiguity of a candidate. This model minimized AIC and BIC more than any other model that we tried.

Finally, we included an interaction term between ideology and the candidate's party. This was based on Figure 4, which shows a relationship between the candidate’s party and ideology. Therefore, we have a model, where i represents district and j represents the candidate:

This will create a model that has an AIC of approximately -42.68, and a BIC of approximately -2.209. These are remarkably small values for AIC and BIC compared to the other models that we tried to make. Some models had slight improvements on AIC, but these results were often only a small improvement on AIC, and a significant reduction in effectiveness for BIC. In addition, this model tested the relationships that we wanted to investigate.

\begin{align}
Y_{ij}&=[\alpha_{0}+\beta_{0}\text{ideology}_{ij}+\beta_{1}\text{ideology}_{ij}\times\text{democrat}_{ij}+\beta_{2}\text{democrat}_{ij}+\beta_{3}\text{incumbent}_{ij}\\
&+\beta_{4}\text{Dem.Heterogeneity}_{i}+\beta{5}\text{TotalIssuePages}_{ij}]+[u_{i}+\epsilon_{ij}]
\end{align}

where, $u_i\sim N(0,\sigma^2_u)$ and $\epsilon_{ij}\sim N(0,\sigma^2)$.  

 $\alpha_0$ is the variable that indicates the ambiguity we expect for challengers who are republican, in districts with a value of demographic heterogeneity of zero and there are no pages. This is not a valuable measure, because we do not expect there to be a case where this reflects a real scenario.

$\beta_0$ represents the expected change that we would see in ambiguity scores for a one-point increase in ideology score for republicans, all other variables held constant. Likewise, $\beta_1$ represents the expected change that we would see in ambiguity scores for a one-point increase in ideology score for democrats, all other variables held constant. $\beta_2$ represents the difference between democrats and republicans for candidates with ideology scores of zero, all else held equal.

$\beta_3$ represents the expected change that we would see in ambiguity scores for incumbents, when compared to challengers, all else held equal. $\beta_4$ represents the change that we would expect in ambiguity scores as the Demographic Heterogeneity increases by a score of one within a district. Finally, $\beta_5$ represents the change in ambiguity scores as the total number of issue pages is increased by one for a candidate, all else held equal.





### Frequentist Results  


```{r}
M5<-lmer(data=pol,ambiguity_fix~ideology*democrat+incumbent+demHeterogeneity+totalIssuePages+(1|distID),REML=TRUE)
#summary(M5)
```


```{r, fig.cap="Variance and Covariance Estimates for Random Effects",fig.width=6}
kable(as.data.frame(VarCorr(M5)),digits=4,col.names = c("Group","Var. 1","Var. 2","Variance","Standard Deviation"),format="pipe",caption="Variance and Covariance Estimates for Random Effects")
```


```{r, fig.cap="Estimates of Fixed Effects",fig.width=6}
kable(summary(M5)$coefficients,digits = 4,format="pipe",caption = "Estimates of Fixed Effects")
```


The model estimates that Republican challengers with no issue pages, an ideology of 0, running in a district with no demographic heterogeneity will have communication ambiguity of -0.749305. For each additional point in ideology for Republicans, the model estimates a 0.012945 point increase in communication ambiguity after accounting for all other fixed effects. However, it should also be noted that this result is not statistically significant, with a large p-value of 0.45. Democrats are estimated to have communication ambiguity 0.047871 less than Republicans after accounting for all other fixed effects. Incumbents are estimated to have communication ambiguity 0.117505 less than challengers after accounting for all other fixed effects. For each additional point in a district's demographic heterogeneity, the model estimates a 0.815260 point increase in communication ambiguity after accounting for all other fixed effects. For each additional issue page on a candidate's website, the model estimates a 0.015542 point decrease in communication ambiguity after accounting for all other fixed effects. For each additional point in ideology, the model estimates that Democrats have an additional 0.100959 point increase in communication ambiguity over Republicans. The model estimates that the standard deviation in communication ambiguity across districts is 0.007862 after accounting for all fixed effects.



### Bayesian Results



### Discussion and Conclusions   

In this paper, we saw that ambiguity does differ with an interaction between a candidate's party identification and their ideology. Given a p-value of roughly zero for `ideology:democrat`, we feel confident that Republicans have a different change in ambiguity depending on ideology than Democrats. The estimate for this variable is 0.100959 and indicates that Democrats become more ambiguous per increase in ideology than Republicans. This connects to the theory from Milita, Simas, Ryan, and Krupnikov (2017) that depending on party, candidates may feel more comfortable with the idea that voters rely on party heuristics when faced with ambiguous language. Our results add evidence to Shesle's research that incumbents tend to have greater clarity than challengers overall, potentially because they are constrained by past voting behavior, greater media scrutiny, and previous position statements (Shepsle, 1972). The model estimate for `incumbent` is -0.117505, with a p-value of nearly zero, indicating we can be very confident incumbents tend to be more clear. Our model does agree with and provide further evidence to Chap et al. that as a district becomes more demographically heterogeneous, ambiguity increases. We see an estimate of 0.815260 for `demHeterogeneity` and a p-value that was roughly zero. Finally, we see statistical evidence of a relationship between the number of issue pages on a candidate's website and less ambiguous language, albeit not a dramatic decrease. The estimate for `totalIssuePages` is -0.015542 with a p-value of approximately zero. This answers our last research question following (Drucker et al. 2004) who concluded that there is little political incentive to address an issue ambiguously when avoidance will do. When candidates do address an issue, they are more likely to be clear about it.

Studies of ambiguity in political communication could benefit from a greater understanding of political success with our research questions in mind. Since the primary driver for most politicians is to win the election, it would be fascinating to consider vote shares between candidates. Other interesting directions for this research would be to compile data from many more election cycles to track individual candidates, parties, and other variables over time. This could add nuance to interactions between party and ideology depending on broader public opinion on generic ballots and control of the white house. For example, a party in 2014 may have been okay with voters relying on heuristics, if the public attitudes towards them were favorable, and opted for more ambiguous language. But generally opinions change for parties and candidates, especially depending on who controls the white house.

It is clear that certain candidate- and district-level factors lead to more ambiguous political communications. Going forward, it may be beneficial to our democracy to consider what, if any, constituent-induced factors contribute to the ambiguity of a candidate's communications. That research may give greater agency to voters to shape political discussions and help strengthen our democracy.


\newpage 

### References

British Broadcasting Corporation. (2014, November 11). The teenage soldiers of World War One. BBC News. Retrieved May 20, 2022, from https://www.bbc.com/news/magazine-29934965 

Brown University. (2021, September 1). Costs of the 20-Year War on terror: $8 trillion and 900,000 deaths. Brown University. Retrieved May 17, 2022, from https://www.brown.edu/news/2021-09-01/costsofwar 

Gallup, G. (1942). How Important is Public Opinion in Time of War. Proceedings of the American Philosophical Society, 85(5), 440–444.

Gelpi, C., Feaver, P., & Reifler, J. (2009). Paying the Human Costs of War. Princeton University Press.

Haas, R. (1994). Intervention: The Use of American Military Force in the Post-Cold War World. Carnegie Endowment for International Peace.

Jentleson, B. W. (1992). The Pretty Prudent Public: Post Post-Vietnam American Opinion on the Use of Military Force. International Studies Quarterly, 36(1), 49–73. https://doi.org/10.2307/2600916

Khazan, O. (2013, September 4). What are the big factors determining whether Americans support war? The Atlantic. Retrieved March 29, 2022, from https://www.theatlantic.com/politics/archive/2013/09/what-are-the-big-factors-determining-whether-americans-support-war/279290/-sets-box-office-record 


Lieberfeld, D. (2008). What Makes An Effective Antiwar Movement? Theme-Issue Introduction. International Journal of Peace Studies, 13(1), 1–14. http://www.jstor.org/stable/41852966




\newpage

## Appendix

[1] For more information on the specific methodology of their textual analysis, the original paper of Wordscores may be of interest. From that paper: "The ultimate methodological price to be paid for the benefits of a posteriori interpretation is the lack of any objective criterion for deciding between rival spatial interpretations, in situations in which the precise choice of interpretation can be critical to the purpose at hand. The price for taking the a priori route, on the other hand, is the need to accept take-it-or-leave-it propositions about the number and substantive meaning of the policy dimensions under investigation. Using the a priori method we introduce here, however, this price can be drastically reduced. This is because, once texts have been processed, it is very easy to re-estimate their positions on a new a priori dimension in which the analyst might be interested. For this reason we concentrate here on estimating positions on a priori policy dimensions."

[2] In some of our data wrangling, we chose to switch the sign of the ambiguity variable in the dataset. Previously, it was higher scores that indicated higher clarity. However, because ambiguity is the response variable and the variable is titled 'ambiguity' we thought it made more sense for analysis to switch these around. 

This is important for two reasons: First, any outside analysis that a person sees about Wordscores software will seem to draw the opposite conclusions; this is not the case, but the language prioritizes clarity over ambiguity. And second, this mutation was made after beginning work on our graphics and may not be reflected as clearly as possible in our analysis. This footnote serves to hopefully dispel any confusion. 

```{r,include=FALSE}
Check <- pol_read %>% group_by(distID) %>% summarize(MeanLean=mean(distLean),MeanDemHet=mean(demHeterogeneity),MeanAttHet=mean(attHeterogeneity))

Check2 <-left_join(pol_read,Check)
Check3 <-Check2 %>% mutate(DiffLean=distLean-MeanLean,Diffatt=attHeterogeneity-MeanAttHet,DiffDemHet=demHeterogeneity-MeanDemHet)

Check3 %>% filter(DiffLean!=0|Diffatt!=0|DiffDemHet!=0)
#this was a check done to make sure that none of our level 2 data was different within the same district. 
```